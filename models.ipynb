{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, model, training_loader, info_per_batch, loss_fn, optimizer):\n",
    "   running_loss = 0.\n",
    "   last_loss = 0.\n",
    "   st = time.time()\n",
    "\n",
    "   # Here, we use enumerate(training_loader) instead of\n",
    "   # iter(training_loader) so that we can track the batch\n",
    "   # index and do some intra-epoch reporting\n",
    "   for i, data in enumerate(training_loader):\n",
    "      # Every data instance is an input + label pair\n",
    "      inputs, labels = data\n",
    "\n",
    "      # Zero your gradients for every batch!\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Make predictions for this batch\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Compute the loss and its gradients\n",
    "      loss = loss_fn(outputs, labels)\n",
    "      loss.backward()\n",
    "\n",
    "      # Adjust learning weights\n",
    "      optimizer.step()\n",
    "\n",
    "      # Gather data and report\n",
    "      running_loss += loss.item()\n",
    "      if i % 1000 == 999:\n",
    "         end = time.time()\n",
    "         last_loss = running_loss / 1000  # loss per batch\n",
    "         last_time = (end - st) / 1000  # time per batch\n",
    "         print('batch {} loss: {}'.format(i + 1, last_loss))\n",
    "         running_loss = 0.\n",
    "         \n",
    "         _info_per_batch = {}\n",
    "         _info_per_batch['epoch'] = epoch_index\n",
    "         _info_per_batch['batch'] = i + 1\n",
    "         _info_per_batch['mean_loss'] = last_loss\n",
    "         _info_per_batch['mean_time'] = last_time\n",
    "         _info_per_batch['time_image_batch'] = last_time / BATCH_SIZE\n",
    "         info_per_batch.append(_info_per_batch)\n",
    "\n",
    "         st = time.time()\n",
    "         \n",
    "\n",
    "   return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, training_loader, info_per_batch, loss_fn, optimizer, validation_loader):\n",
    "   epoch_number = 0\n",
    "\n",
    "   best_vloss = 1_000_000.\n",
    "\n",
    "   for epoch in range(EPOCHS):\n",
    "      print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "      # Make sure gradient tracking is on, and do a pass over the data\n",
    "      model.train(True)\n",
    "      avg_loss = train_one_epoch(epoch_number + 1, model, training_loader, info_per_batch, loss_fn, optimizer)\n",
    "\n",
    "      # We don't need gradients on to do reporting\n",
    "      model.train(False)\n",
    "\n",
    "      running_vloss = 0.0\n",
    "      for i, vdata in enumerate(validation_loader):\n",
    "         vinputs, vlabels = vdata\n",
    "         voutputs = model(vinputs)\n",
    "         vloss = loss_fn(voutputs, vlabels)\n",
    "         running_vloss += vloss\n",
    "\n",
    "      avg_vloss = running_vloss / (i + 1)\n",
    "      print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "\n",
    "      # Salva o melhor modelo\n",
    "      # if avg_vloss < best_vloss:\n",
    "      #    best_vloss = avg_vloss\n",
    "      #    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "      #    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "   def __init__(self):\n",
    "      super(LeNet, self).__init__()\n",
    "      # Definição de cada camada da rede neural como atributo da classe\n",
    "      self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "      self.pool = nn.MaxPool2d(2, 2)\n",
    "      self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "      self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "      self.fc2 = nn.Linear(120, 84)\n",
    "      self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "   def forward(self, x):\n",
    "      # Computação dos passos de uma rede neural até a saída produzida\n",
    "      x = self.pool(F.relu(self.conv1(x)))\n",
    "      x = self.pool(F.relu(self.conv2(x)))\n",
    "      x = x.view(-1, 16 * 4 * 4)\n",
    "      x = F.relu(self.fc1(x))\n",
    "      x = F.relu(self.fc2(x))\n",
    "      x = self.fc3(x)\n",
    "      return x\n",
    "\n",
    "   def num_flat_features(self, x):\n",
    "      size = x.size()[1:]\n",
    "      return np.prod(size)\n",
    "   \n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_set = torchvision.datasets.FashionMNIST(\n",
    "    './data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST(\n",
    "    './data', train=False, transform=transform, download=True)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_per_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "batch 1000 loss: 1.9247333088442684\n",
      "batch 2000 loss: 0.8553839588742703\n",
      "batch 3000 loss: 0.7070250067040325\n",
      "batch 4000 loss: 0.6494240892494563\n",
      "batch 5000 loss: 0.6108041729792021\n",
      "batch 6000 loss: 0.549041643061908\n",
      "batch 7000 loss: 0.5509025329215219\n",
      "batch 8000 loss: 0.5206029276913032\n",
      "batch 9000 loss: 0.49983676334423943\n",
      "batch 10000 loss: 0.47071835911343807\n",
      "batch 11000 loss: 0.4580784262334928\n",
      "batch 12000 loss: 0.4284323982937494\n",
      "batch 13000 loss: 0.4293992165924865\n",
      "batch 14000 loss: 0.4209640056388453\n",
      "batch 15000 loss: 0.41167909247224455\n",
      "LOSS train 0.41167909247224455 valid 0.4281473159790039\n",
      "EPOCH 1:\n",
      "batch 1000 loss: 0.39714858395492775\n",
      "batch 2000 loss: 0.3802399608445121\n",
      "batch 3000 loss: 0.4071223537927726\n",
      "batch 4000 loss: 0.3690648090149043\n",
      "batch 5000 loss: 0.3687860967501765\n",
      "batch 6000 loss: 0.3920790032781661\n",
      "batch 7000 loss: 0.36273490110854617\n",
      "batch 8000 loss: 0.3779130775448866\n",
      "batch 9000 loss: 0.4044479777190136\n",
      "batch 10000 loss: 0.34476273694064\n",
      "batch 11000 loss: 0.3542223175354884\n",
      "batch 12000 loss: 0.3714814358811855\n",
      "batch 13000 loss: 0.3281453086367546\n",
      "batch 14000 loss: 0.3335811241574702\n",
      "batch 15000 loss: 0.34982813866174545\n",
      "LOSS train 0.34982813866174545 valid 0.3540658950805664\n"
     ]
    }
   ],
   "source": [
    "train_epochs(model, training_loader, info_per_batch, loss_fn, optimizer, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "        self.fc = nn.Linear(192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d927377e6494403bfb7440cf536e4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b84b89eef694a65b2843cbff83295d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a7117c52bb438db5cf4db39fdd0f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5d09040d44403195cda6305f247b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "training_set = torchvision.datasets.MNIST(\n",
    "    './data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST(\n",
    "    './data', train=False, transform=transform, download=True)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_per_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "batch 1000 loss: 0.915266542147845\n",
      "batch 2000 loss: 0.32670243506785485\n",
      "batch 3000 loss: 0.30372809440642595\n",
      "batch 4000 loss: 0.246821152283228\n",
      "batch 5000 loss: 0.22014371296297758\n",
      "batch 6000 loss: 0.20948628368793287\n",
      "batch 7000 loss: 0.22012425211054507\n",
      "batch 8000 loss: 0.17692193558806502\n",
      "batch 9000 loss: 0.1946313565367018\n",
      "batch 10000 loss: 0.18857012452167693\n",
      "batch 11000 loss: 0.17499071596813157\n",
      "batch 12000 loss: 0.17122693366328895\n",
      "batch 13000 loss: 0.183746412788023\n",
      "batch 14000 loss: 0.15741507074429684\n",
      "batch 15000 loss: 0.1538833634554976\n",
      "LOSS train 0.1538833634554976 valid 0.14506042003631592\n",
      "EPOCH 1:\n",
      "batch 1000 loss: 0.14829538930589478\n",
      "batch 2000 loss: 0.16545096516517516\n",
      "batch 3000 loss: 0.1331894893800636\n",
      "batch 4000 loss: 0.16671721210842952\n",
      "batch 5000 loss: 0.16274361704062176\n",
      "batch 6000 loss: 0.1515509745500749\n",
      "batch 7000 loss: 0.1369479431087384\n",
      "batch 8000 loss: 0.13778779771183328\n",
      "batch 9000 loss: 0.14997765749251993\n",
      "batch 10000 loss: 0.13678151208540293\n",
      "batch 11000 loss: 0.14309710760763847\n",
      "batch 12000 loss: 0.12825745704256405\n",
      "batch 13000 loss: 0.14270608396156967\n",
      "batch 14000 loss: 0.11675180764495235\n",
      "batch 15000 loss: 0.14139073005291722\n",
      "LOSS train 0.14139073005291722 valid 0.11909875273704529\n"
     ]
    }
   ],
   "source": [
    "train_epochs(model, training_loader, info_per_batch, loss_fn, optimizer, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "   def __init__(self):\n",
    "      super(CustomCNN, self).__init__()\n",
    "      # 2 camada convolucionais, 1 camada de pooing e 1 camada linear\n",
    "      self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5,5))\n",
    "      self.batchN1 = nn.BatchNorm2d(num_features=6)\n",
    "      self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=(5,5))\n",
    "      self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "      self.batchN2 = nn.BatchNorm1d(num_features=120)\n",
    "      self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "      self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "   def forward(self, x):\n",
    "      # hidden conv layer \n",
    "      x = self.conv1(x)\n",
    "      x = F.max_pool2d(input=x, kernel_size=2, stride=2)\n",
    "      x = F.relu(x)\n",
    "      x = self.batchN1(x)\n",
    "      \n",
    "      # hidden conv layer\n",
    "      x = self.conv2(x)\n",
    "      x = F.max_pool2d(input=x, kernel_size=2, stride=2)\n",
    "      x = F.relu(x)\n",
    "      \n",
    "      # flatten\n",
    "      x = x.reshape(-1, 12*4*4)\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.batchN2(x)\n",
    "      x = self.fc2(x)\n",
    "      x = F.relu(x)\n",
    "      \n",
    "      # output\n",
    "      x = self.out(x)\n",
    "      \n",
    "      return x\n",
    "   \n",
    "model = CustomCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "training_set = torchvision.datasets.MNIST(\n",
    "    './data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST(\n",
    "    './data', train=False, transform=transform, download=True)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_per_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "batch 1000 loss: 1.0825842409096658\n",
      "batch 2000 loss: 0.4761379439672455\n",
      "batch 3000 loss: 0.4385914647215977\n",
      "batch 4000 loss: 0.3493854379858822\n",
      "batch 5000 loss: 0.2890595387509093\n",
      "batch 6000 loss: 0.33691676146443933\n",
      "batch 7000 loss: 0.2976871873548953\n",
      "batch 8000 loss: 0.2882055274118902\n",
      "batch 9000 loss: 0.3216075420464622\n",
      "batch 10000 loss: 0.26523406780627556\n",
      "batch 11000 loss: 0.271908985201444\n",
      "batch 12000 loss: 0.24736220272514037\n",
      "batch 13000 loss: 0.2523748410275439\n",
      "batch 14000 loss: 0.22459345625020796\n",
      "batch 15000 loss: 0.2487560393444437\n",
      "LOSS train 0.2487560393444437 valid 0.08176848292350769\n",
      "EPOCH 1:\n",
      "batch 1000 loss: 0.2439960263131943\n",
      "batch 2000 loss: 0.24325198863062542\n",
      "batch 3000 loss: 0.2378574885253329\n",
      "batch 4000 loss: 0.2099797116574482\n",
      "batch 5000 loss: 0.2248728931088408\n",
      "batch 6000 loss: 0.21331114551139763\n",
      "batch 7000 loss: 0.4714641777734505\n",
      "batch 8000 loss: 0.351691224899143\n",
      "batch 9000 loss: 0.3083083811714314\n",
      "batch 10000 loss: 0.2742406782631879\n",
      "batch 11000 loss: 0.23631797617615666\n",
      "batch 12000 loss: 0.2530604254403734\n",
      "batch 13000 loss: 0.2352095016342937\n",
      "batch 14000 loss: 0.20506282307230866\n",
      "batch 15000 loss: 0.26480053030495765\n",
      "LOSS train 0.26480053030495765 valid 0.19200682640075684\n"
     ]
    }
   ],
   "source": [
    "train_epochs(model, training_loader, info_per_batch, loss_fn, optimizer, validation_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
